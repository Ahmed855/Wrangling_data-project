{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the Data to a CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "#import cerberus\n",
    "import re\n",
    "import schema\n",
    "\n",
    "\n",
    "\n",
    "OSM_PATH = \"houston_texas.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "\n",
    "# Clean and shape node or way XML element to Python dict\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS,\n",
    "                  way_attr_fields=WAY_FIELDS, prob_ch=PROBLEMCHARS,\n",
    "                  default_tag_type='regular'):\n",
    "    \n",
    "    tag_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []\n",
    "    count = 0\n",
    "    if element.tag == 'node':\n",
    "        tagfields = node_attr_fields\n",
    "    elif element.tag == 'way':\n",
    "        tagfields = way_attr_fields\n",
    "    \n",
    "    if element.tag == 'node' or 'way':\n",
    "        for attrib in element.attrib:\n",
    "            if attrib in tagfields:\n",
    "                tag_attribs[attrib] = element.attrib[attrib]\n",
    "                \n",
    "                \n",
    "                \n",
    "    for subelem in element:\n",
    "        if subelem.tag == 'tag' and prob_ch.match(subelem.attrib['k']) == None:\n",
    "            tag = {}\n",
    "            tag['id'] = tag_attribs['id']\n",
    "            if subelem.attrib[\"k\"] == 'addr:street':\n",
    "                        tag[\"value\"] = update_name(subelem.attrib[\"v\"], mapping, regex )\n",
    "            else:\n",
    "                        tag[\"value\"] = subelem.attrib[\"v\"]\n",
    "            key = subelem.attrib['k']\n",
    "            tag['key'] = key[key.find(':') + 1:]\n",
    "            if ':' in key:\n",
    "                tag['type'] = key[:key.find(':')]\n",
    "            else:\n",
    "                tag['type'] = default_tag_type\n",
    "            tags.append(tag)\n",
    "        elif subelem.tag == 'nd':\n",
    "            way_node = {}\n",
    "            way_node['id'] = tag_attribs['id']\n",
    "            way_node['node_id'] = subelem.attrib['ref']\n",
    "            way_node['position'] = count\n",
    "            count += 1\n",
    "            way_nodes.append(way_node)\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        return {'node': tag_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        return {'way': tag_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "    \n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(OSMFILE, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(OSMFILE, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_strings = (\n",
    "            \"{0}: {1}\".format(k, v if isinstance(v, str) else \", \".join(v))\n",
    "            for k, v in errors.iteritems()\n",
    "        )\n",
    "        raise cerberus.ValidationError(\n",
    "            message_string.format(field, \"\\n\".join(error_strings))\n",
    "        )\n",
    "\n",
    "\n",
    "# Extend csv.DictWriter to handle Unicode input\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "            \n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_map(OSM_PATH, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chicking the files Size :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def file_size_mb(filePath): \n",
    "    return float(os.path.getsize(filePath)) /(1024 * 1024)\n",
    "\n",
    "print 'The file size for houston_texas.osm is {} MB'.format(file_size_mb(\"houston_texas.osm\"))\n",
    "print 'The file size for h_db.db is {} MB'.format(file_size_mb(\"h_db.db\"))\n",
    "print 'The file size for nodes.csv  is {} MB'.format(file_size_mb(\"nodes.csv\"))\n",
    "print 'The file size for ways.csv is {} MB'.format(file_size_mb(\"ways.csv\"))\n",
    "print 'The file size for ways_nodes.csv is {} MB'.format(file_size_mb(\"ways_nodes.csv\"))\n",
    "print 'The file size for nodes_tags.csv is {} MB'.format(file_size_mb(\"nodes_tags.csv\"))\n",
    "print 'The file size for ways_tags.csv is {} MB'.format(file_size_mb(\"ways_tags.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def query(query):\n",
    "    conn = sqlite3.connect('h_db.db')\n",
    "    data = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of tags in Nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query(\"SELECT COUNT(*) FROM nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### checking out the number of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query(\"SELECT COUNT(*) FROM ways\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  How many unique users have created and updated this dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query(\n",
    "    '''\n",
    "SELECT count(DISTINCT(uid)) AS \"Unique Users\"\n",
    "FROM (SELECT uid FROM nodes\n",
    "      UNION SELECT uid FROM ways) AS elements;\n",
    "      '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The top 5 streets that have the more node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query( '''\n",
    "SELECT ways_tags.value, COUNT(*) FROM ways_tags\n",
    "WHERE ways_tags.key = 'name' AND ways_tags.type = 'regular' \n",
    "GROUP BY ways_tags.value ORDER BY COUNT(*) DESC LIMIT 6;\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The count of place of worships for christian is assure that Texas mybe somehowe a religious state,however as a muslime and lived there for few years I can easly found Mosque where I can go and pary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query ( '''\n",
    "SELECT nodes_tags.value, COUNT(*) as Count\n",
    "FROM nodes_tags \n",
    "JOIN\n",
    "    (SELECT DISTINCT(id)\n",
    "    FROM nodes_tags\n",
    "    WHERE value='place_of_worship') as Sub\n",
    "ON nodes_tags.id=Sub.id\n",
    "WHERE nodes_tags.key='religion'\n",
    "GROUP BY nodes_tags.value\n",
    "ORDER BY Count DESC;\n",
    "'''\n",
    "   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Who made the most of midification for houston map !?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query (\n",
    "    '''\n",
    "SELECT DISTINCT nodes.user, COUNT(*)\n",
    "FROM nodes\n",
    "GROUP BY nodes.uid\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 6;\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I used to be a customer of Bank of America,However it seems not the most puplor bank in the city :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqldata (\n",
    "    '''\n",
    "    SELECT nodes_tags.value, COUNT(*) as num\n",
    "        FROM nodes_tags\n",
    "            JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value='bank') i\n",
    "            ON nodes_tags.id=i.id\n",
    "        WHERE nodes_tags.key='name'\n",
    "        GROUP BY nodes_tags.value\n",
    "        ORDER BY num DESC\n",
    "        LIMIT 6;\n",
    "        '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Ideas to Improve OSM data\n",
    "\n",
    "To improve OSM since it's an open source, it might be connected with other tools like Google Maps API espicilly that Google Maps API We have  more than one billion global monthly active users in over 200 countries, this amont of data will give  accurate real-time information for mapping around the world .\n",
    "\n",
    "Also, another idea to improve the data set we could use  the popular game Pokemon Go to improve the dataset.Since the game got so popular and informations and that we can get out of can be really so helpfull to improve street mape .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and benefits as well as some anticipated problems .\n",
    "\n",
    "First of all, after this project I truly believe the saying that the data analyst is spending almost %75 if his time in cleaning the data! Cleaning such data is not smooth as analysing cleaned dataset. Secondlly, if though I had lived in Houston for some time, after cleaning the street Map it seems now I know a lot more about the city which means how valube the data that is available online to get know something better.\n",
    "By going all the way down in this process for auditing and cleaning the data, one benefits we can get from that, now we are sure and accurate about the information we have stated about the city of Houston. Cleaning the data can give deep information that not easily can be notice. Also it was interesting to me how many people have worked to improve and do some change in the street mapping. Some anticipated problems can come because it’s difficult to know exactly if a place should be an “area” or just a “way”, or another place should be an “street\" or it's \"Aven\".So maybe in the future by clearly defining the \"way\" \"area\" \"street\" \"aven\" \"road\" that will help a lot improving the datasets.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
